{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5467545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c15c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALISIS SENTIMEN: KASUS SALAH TANGKAP BJORKA\n",
      "Tahap 2: Pelabelan Sentimen dengan IndoBERT\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ANALISIS SENTIMEN: KASUS SALAH TANGKAP BJORKA\")\n",
    "print(\"Tahap 2: Pelabelan Sentimen dengan IndoBERT\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7abc5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] MEMUAT DATA...\n",
      "----------------------------------------------------------------------\n",
      "✓ Data berhasil dimuat: 2177 komentar\n",
      "✓ Kolom: ['clean_comment', 'char_length', 'word_count']\n"
     ]
    }
   ],
   "source": [
    "# 1. LOAD DATA\n",
    "print(\"\\n[1] MEMUAT DATA...\")\n",
    "print(\"-\"*70)\n",
    "df = pd.read_csv('youtube_comments_with_stats.csv')\n",
    "print(f\"✓ Data berhasil dimuat: {len(df)} komentar\")\n",
    "print(f\"✓ Kolom: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32aacfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] SETUP MODEL INDOBERT...\n",
      "----------------------------------------------------------------------\n",
      "⏳ Downloading model IndoBERT (ini mungkin butuh waktu di run pertama)...\n",
      "✓ Model berhasil dimuat!\n",
      "✓ Model: mdhugol/indonesia-bert-sentiment-classification\n",
      "✓ Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 2. SETUP MODEL INDOBERT\n",
    "print(\"\\n[2] SETUP MODEL INDOBERT...\")\n",
    "print(\"-\"*70)\n",
    "print(\"⏳ Downloading model IndoBERT (ini mungkin butuh waktu di run pertama)...\")\n",
    "\n",
    "# Pilihan model IndoBERT untuk sentiment analysis\n",
    "# Model: indobenchmark/indobert-base-p1 (fine-tuned untuk sentiment)\n",
    "model_name = \"mdhugol/indonesia-bert-sentiment-classification\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    print(\"✓ Model berhasil dimuat!\")\n",
    "    print(f\"✓ Model: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    print(\"\\n⚠️ Alternatif: Coba model lain jika error\")\n",
    "    print(\"   - IndoNLU: 'indonlu/indobert-base-p1'\")\n",
    "    print(\"   - mBERT multilingual: 'bert-base-multilingual-uncased'\")\n",
    "    exit()\n",
    "\n",
    "# Check if GPU available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"✓ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d18fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] PREPROCESSING DATA...\n",
      "----------------------------------------------------------------------\n",
      "✓ Dataset created: 2177 samples\n",
      "✓ Batch size: 16\n",
      "✓ Number of batches: 137\n"
     ]
    }
   ],
   "source": [
    "# 3. PREPROCESSING UNTUK BATCH PREDICTION\n",
    "print(\"\\n[3] PREPROCESSING DATA...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, comments, tokenizer, max_length=128):\n",
    "        self.comments = comments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        comment = str(self.comments[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# Buat dataset dan dataloader\n",
    "batch_size = 16  # Sesuaikan dengan RAM/GPU Anda\n",
    "dataset = CommentDataset(df['clean_comment'].values, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✓ Dataset created: {len(dataset)} samples\")\n",
    "print(f\"✓ Batch size: {batch_size}\")\n",
    "print(f\"✓ Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a09be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] MELAKUKAN PREDIKSI SENTIMEN...\n",
      "----------------------------------------------------------------------\n",
      "⏳ Memproses... (ini akan memakan waktu tergantung jumlah data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 137/137 [04:00<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Prediksi selesai!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. PREDIKSI SENTIMEN\n",
    "print(\"\\n[4] MELAKUKAN PREDIKSI SENTIMEN...\")\n",
    "print(\"-\"*70)\n",
    "print(\"⏳ Memproses... (ini akan memakan waktu tergantung jumlah data)\")\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Softmax untuk mendapatkan probability\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Ambil prediksi (class dengan probability tertinggi)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        probabilities.extend(probs.cpu().numpy())\n",
    "\n",
    "print(\"\\n✓ Prediksi selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6470dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5] MAPPING LABEL SENTIMEN...\n",
      "----------------------------------------------------------------------\n",
      "✓ Label sentimen berhasil ditambahkan!\n",
      "✓ Kolom baru: sentiment_label, sentiment_score, confidence, prob_*\n"
     ]
    }
   ],
   "source": [
    "# 5. MAPPING LABEL SENTIMEN\n",
    "print(\"\\n[5] MAPPING LABEL SENTIMEN...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Mapping label (sesuaikan dengan model yang digunakan)\n",
    "# Untuk model mdhugol/indonesia-bert-sentiment-classification:\n",
    "# 0 = Positive, 1 = Neutral, 2 = Negative\n",
    "sentiment_map = {\n",
    "    0: 'positive',\n",
    "    1: 'neutral', \n",
    "    2: 'negative'\n",
    "}\n",
    "\n",
    "df['sentiment_label'] = [sentiment_map[pred] for pred in predictions]\n",
    "df['sentiment_score'] = predictions\n",
    "\n",
    "# Ambil confidence score (probability tertinggi)\n",
    "df['confidence'] = [max(prob) for prob in probabilities]\n",
    "\n",
    "# Probability untuk setiap class\n",
    "prob_df = pd.DataFrame(probabilities, columns=['prob_positive', 'prob_neutral', 'prob_negative'])\n",
    "df = pd.concat([df, prob_df], axis=1)\n",
    "\n",
    "print(\"✓ Label sentimen berhasil ditambahkan!\")\n",
    "print(f\"✓ Kolom baru: sentiment_label, sentiment_score, confidence, prob_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1959260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6] ANALISIS HASIL PELABELAN\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Distribusi Sentimen:\n",
      "  Negative.......   1280 (58.80%)\n",
      "  Neutral........    512 (23.52%)\n",
      "  Positive.......    385 (17.68%)\n",
      "\n",
      "Rata-rata Confidence Score: 0.8984\n",
      "Min Confidence Score: 0.3688\n",
      "Max Confidence Score: 0.9983\n",
      "\n",
      "Rata-rata Confidence per Sentimen:\n",
      "  Positive....... 0.8616\n",
      "  Neutral........ 0.8633\n",
      "  Negative....... 0.9234\n"
     ]
    }
   ],
   "source": [
    "# 6. ANALISIS HASIL\n",
    "print(\"\\n[6] ANALISIS HASIL PELABELAN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nDistribusi Sentimen:\")\n",
    "sentiment_counts = df['sentiment_label'].value_counts()\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {sentiment.capitalize():.<15} {count:>6} ({percentage:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nRata-rata Confidence Score: {df['confidence'].mean():.4f}\")\n",
    "print(f\"Min Confidence Score: {df['confidence'].min():.4f}\")\n",
    "print(f\"Max Confidence Score: {df['confidence'].max():.4f}\")\n",
    "\n",
    "# Confidence berdasarkan sentimen\n",
    "print(\"\\nRata-rata Confidence per Sentimen:\")\n",
    "for sentiment in sentiment_map.values():\n",
    "    avg_conf = df[df['sentiment_label'] == sentiment]['confidence'].mean()\n",
    "    print(f\"  {sentiment.capitalize():.<15} {avg_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a79cba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7] CONTOH HASIL PREDIKSI\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "🟢 CONTOH KOMENTAR POSITIVE (5 teratas):\n",
      "\n",
      "Komentar #801 (Confidence: 0.9972)\n",
      "  gaya bicara penyampaiann aa teguh enak banget...\n",
      "\n",
      "Komentar #507 (Confidence: 0.9972)\n",
      "  mantap...\n",
      "\n",
      "Komentar #1435 (Confidence: 0.9972)\n",
      "  422 sumpah estetik banget transisi halus warnawarnanya bikin adem 855dan musik latar cocok banget...\n",
      "\n",
      "Komentar #2084 (Confidence: 0.9972)\n",
      "  keren...\n",
      "\n",
      "Komentar #865 (Confidence: 0.9969)\n",
      "  istimewa...\n",
      "\n",
      "🟡 CONTOH KOMENTAR NEUTRAL (5 teratas):\n",
      "\n",
      "Komentar #941 (Confidence: 0.9983)\n",
      "  nirvana090 cek mutasi uang kmna...\n",
      "\n",
      "Komentar #1812 (Confidence: 0.9981)\n",
      "  tau undang om dedy muncul ig...\n",
      "\n",
      "Komentar #1922 (Confidence: 0.9980)\n",
      "  bang ded coba undang pace komputer...\n",
      "\n",
      "Komentar #1216 (Confidence: 0.9980)\n",
      "  3621 deddy masuk kantor cyber tv pake video ala matrix...\n",
      "\n",
      "Komentar #1854 (Confidence: 0.9976)\n",
      "  ngk undang pakar telematika sang ungkap video gambar pnas...\n",
      "\n",
      "🔴 CONTOH KOMENTAR NEGATIVE (5 teratas):\n",
      "\n",
      "Komentar #2098 (Confidence: 0.9983)\n",
      "  malu dengernya ah simpan hati aja aib...\n",
      "\n",
      "Komentar #1943 (Confidence: 0.9983)\n",
      "  sok sok nangkep hacker brantas judol aja ga kelar kelar...\n",
      "\n",
      "Komentar #2127 (Confidence: 0.9982)\n",
      "  bikin malu aja...\n",
      "\n",
      "Komentar #943 (Confidence: 0.9982)\n",
      "  polisi jahat sungguh...\n",
      "\n",
      "Komentar #928 (Confidence: 0.9982)\n",
      "  jahat sungguh emang...\n"
     ]
    }
   ],
   "source": [
    "# 7. CONTOH HASIL PREDIKSI\n",
    "print(\"\\n[7] CONTOH HASIL PREDIKSI\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n🟢 CONTOH KOMENTAR POSITIVE (5 teratas):\")\n",
    "positive_samples = df[df['sentiment_label'] == 'positive'].nlargest(5, 'confidence')\n",
    "for idx, row in positive_samples.iterrows():\n",
    "    print(f\"\\nKomentar #{idx+1} (Confidence: {row['confidence']:.4f})\")\n",
    "    print(f\"  {row['clean_comment'][:150]}...\")\n",
    "\n",
    "print(\"\\n🟡 CONTOH KOMENTAR NEUTRAL (5 teratas):\")\n",
    "neutral_samples = df[df['sentiment_label'] == 'neutral'].nlargest(5, 'confidence')\n",
    "for idx, row in neutral_samples.iterrows():\n",
    "    print(f\"\\nKomentar #{idx+1} (Confidence: {row['confidence']:.4f})\")\n",
    "    print(f\"  {row['clean_comment'][:150]}...\")\n",
    "\n",
    "print(\"\\n🔴 CONTOH KOMENTAR NEGATIVE (5 teratas):\")\n",
    "negative_samples = df[df['sentiment_label'] == 'negative'].nlargest(5, 'confidence')\n",
    "for idx, row in negative_samples.iterrows():\n",
    "    print(f\"\\nKomentar #{idx+1} (Confidence: {row['confidence']:.4f})\")\n",
    "    print(f\"  {row['clean_comment'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44e7d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8] MENYIMPAN HASIL...\n",
      "----------------------------------------------------------------------\n",
      "✓ Data dengan label sentimen disimpan: 'youtube_comments_labeled.csv'\n",
      "✓ Statistik sentimen disimpan: 'sentiment_statistics.txt'\n",
      "✓ Komentar positive disimpan: 'comments_positive.csv'\n",
      "✓ Komentar neutral disimpan: 'comments_neutral.csv'\n",
      "✓ Komentar negative disimpan: 'comments_negative.csv'\n",
      "\n",
      "======================================================================\n",
      "✓✓✓ TAHAP 2 SELESAI! ✓✓✓\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8. SIMPAN HASIL\n",
    "print(\"\\n[8] MENYIMPAN HASIL...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Simpan data dengan label sentimen\n",
    "output_file = 'youtube_comments_labeled.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"✓ Data dengan label sentimen disimpan: '{output_file}'\")\n",
    "\n",
    "# Simpan statistik sentimen\n",
    "stats_file = 'sentiment_statistics.txt'\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"STATISTIK PELABELAN SENTIMEN - INDOBERT\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"DISTRIBUSI SENTIMEN:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        f.write(f\"{sentiment.capitalize():.<20} {count:>8} ({percentage:>6.2f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\\nTOTAL KOMENTAR: {len(df)}\\n\")\n",
    "    f.write(f\"Rata-rata Confidence: {df['confidence'].mean():.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\nCONFIDENCE SCORE PER SENTIMEN:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for sentiment in sentiment_map.values():\n",
    "        avg_conf = df[df['sentiment_label'] == sentiment]['confidence'].mean()\n",
    "        f.write(f\"{sentiment.capitalize():.<20} {avg_conf:.4f}\\n\")\n",
    "\n",
    "print(f\"✓ Statistik sentimen disimpan: '{stats_file}'\")\n",
    "\n",
    "# Simpan summary per sentimen\n",
    "for sentiment in sentiment_map.values():\n",
    "    sentiment_df = df[df['sentiment_label'] == sentiment]\n",
    "    sentiment_file = f'comments_{sentiment}.csv'\n",
    "    sentiment_df.to_csv(sentiment_file, index=False, encoding='utf-8')\n",
    "    print(f\"✓ Komentar {sentiment} disimpan: '{sentiment_file}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓✓✓ TAHAP 2 SELESAI! ✓✓✓\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
